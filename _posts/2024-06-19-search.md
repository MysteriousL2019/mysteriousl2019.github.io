---
title: Search algorithm-related associations 「搜索算法相关联想」
author: Fangzheng
date: 2024-06-19 19:06:00 +0800
categories: [Generative AI, Artificial Intelligence]
tags: [Algorithm ]
# pin: true
mermaid: true  #code模块
comments: true

math: true
# img_cdn: https://github.com/MysteriousL2019/mysteriousl2019.github.io/tree/master/assets/img/
---
## Search algorithm-related associations「搜索引擎的难题」
* Google早已成为全球最成功的互联网搜索引擎，但这个当前的搜索引擎巨无霸却不是最早的互联网搜索引擎，在Google出现之前，曾出现过许多通用或专业领域搜索引擎。Google最终能击败所有竞争对手，很大程度上是因为它解决了困扰前辈们的最大难题：对搜索结果按重要性排序。而解决这个问题的算法就是PageRank。毫不夸张的说，是PageRank算法成就了Google今天的低位。要理解为什么解决这个难题如此重要，我们先来看一下搜索引擎的核心框架。

## The core framework of search engines「搜索引擎的核心框架」
* 虽然搜索引擎已经发展了很多年，但是其核心却没有太大变化。从本质上说，搜索引擎是一个资料检索系统，搜索引擎拥有一个资料库（具体到这里就是互联网页面），用户提交一个检索条件（例如关键词），搜索引擎返回符合查询条件的资料列表。理论上检索条件可以非常复杂，为了简单起见，我们不妨设检索条件是一至多个以空格分隔的词，而其表达的语义是同时含有这些词的资料（等价于布尔代数的逻辑与）。例如，提交“美食 攻略”，意思就是“给我既含有‘美食’又含有‘攻略’词语的页面”。
* 当然，实际上现在的搜索引擎都是有分词机制的，例如如果以“美食的攻略”为关键词，搜索引擎会自动将其分解为“美食 的 攻略”三个词，而“的”作为停止词（Stop Word）会被过滤掉。关于分词及词权评价算法（如TF-IDF算法）是一个很大的话题，这里就不展开讨论了，为了简单此处可以将搜索引擎想象为一个只会机械匹配词语的检索系统。
* 这样看来，建立一个搜索引擎的核心问题就是两个：1、建立资料库；2、建立一种数据结构，可以根据关键词找到含有这个词的页面。
* 第一个问题一般是通过一种叫爬虫（Spider）的特殊程序实现的（当然，专业领域搜索引擎例如某个学术会议的论文检索系统可能直接从数据库建立资料库），简单来说，爬虫就是从一个页面出发（例如新浪首页），通过HTTP协议通信获取这个页面的所有内容，把这个页面url和内容记录下来（记录到资料库），然后分析页面中的链接，再去分别获取这些链接链向页面的内容，记录到资料库后再分析这个页面的链接……重复这个过程，就可以将整个互联网的页面全部获取下来（当然这是理想情况，要求整个Web是一个强连通（Strongly Connected），并且所有页面的robots协议允许爬虫抓取页面，为了简单，我们仍然假设Web是一个强连通图，且不考虑robots协议）。抽象来看，可以将资料库看做一个巨大的key-value结构，key是页面url，value是页面内容。
* 第二个问题是通过一种叫倒排索引（inverted index）的数据结构实现的，抽象来说倒排索引也是一组key-value结构，key是关键词，value是一个页面编号集合（假设资料库中每个页面有唯一编号），表示这些页面含有这个关键词。本文不详细讨论倒排索引的建立方法。
* 有了上面的分析，就可以简要说明搜索引擎的核心动作了：搜索引擎获取“美食 攻略”查询条件，将其分为“美食”和“攻略”两个词。然后分别从倒排索引中找到“美食”所对应的集合，假设是{1， 3， 6， 8， 11， 15}；“攻略”对应的集合是{1， 6， 10， 11， 12， 17， 20， 22}，将两个集合做交运算（intersection），结果是{1， 6， 11}。最后，从资料库中找出1、6、11对应的页面返回给用户就可以了。
## Second Methods「思路二」：
* 排序时候，引入神经网络，代替pagerank
* 1. 对比学习，
    * 对比学习在解决什么问题？
        * 如何学习representation
        * 解决数据稀疏的问题
        * 如何更好的利用没有label的数据
            * 未打标的数据远远多于打标的数据，不用简直太浪费了，但是要打标又是一个耗时耗力耗钱的事儿
        * 有监督学习的缺点：
            * 泛化能力
            * [Spurious Correlations伪相关](https://www.zhihu.com/question/409100594)
            * [Adversarial Attacks对抗攻击](https://zhuanlan.zhihu.com/p/104532285)
* 2. Attention 机制，实际上就是找到序列的重要程度，利用q和k矩阵作为加权的权重，变化v。(input 都是x，self attention，此时表示搜索的query和内容当作一个输入序列)
## 在推荐系统领域
* 几个基本步骤，推荐系统的主要四个阶段（召回、粗排、精排、重排）

| 阶段   | 特点                                         |
|--------|----------------------------------------------|
| 召回   | 从海量物品中快速找回一部分重要物品            |
| 粗排   | 进行粗略排序，保证一定精准度并减少物品数量    |
| 精排   | 精准地对物品进行个性化排序                    |
| 重排   | 改进用户体验                                  |

| 划分类型 | 描述 | 特点 | 作用 |
|---------|-----|-----|-----|
| **传统划分** | | | |
| 召回 | 根据用户部分特征，从海量的物品库里，快速找回一小部分用户潜在感兴趣的物品。 | 速度快。 | |
| 排序 | 可以融入较多特征，使用复杂模型，来精准地做个性化推荐。 | 结果精准。 | |
| **精细划分** | | | |
| 召回（多路召回） | 根据用户部分特征，从海量的物品库里，快速找回一小部分用户潜在感兴趣的物品。 | | |
| 粗排（可用可不用，根据场景选择） | 通过少量用户和物品特征，简单模型，对召回的结果进行个粗略的排序，保证一定精准的前提下，进一步减少选取的物品数量。 | | 防止用户召回环节返回的物品数量过多，影响排序环节效率。 |
| 精排（重要） | 可以使用任何特征和复杂模型，尽量精准地对物品进行个性化排序。 | | |
| 重排 | 改进用户体验，可以采用各种技术及业务策略（技术产品策略主导），比如去已读、去重、打散、多样性保证、固定类型物品插入等等。 | 技术产品策略主导。 | |


* 比如阿里的[推荐系统个性化重排序](https://arxiv.org/pdf/1904.06813)就是优化了重排的部分

## 大语言模型(LLM)Scaling Laws
* 大模型的Scaling Law是OpenAI在2020年提出的概念[1]，具体如下:
* 对于Decoder-only的模型，计算量C(Flops), 模型参数量N, 数据大小D(token数)，三者满足: $$ C = 6ND$$ 
* 模型的最终性能「主要与」计算量，模型参数量和数据大小三者相关，而与模型的具体结构(层数/深度/宽度)基本无关。
    * * 即固定模型的总参数量，调整层数/深度/宽度，不同模型的性能差距很小，大部分在2%以内
* 对于计算量，模型参数量和数据大小，当不受其他两个因素制约时，模型性能与每个因素都呈现「幂律关系」
*  为了提升模型性能，模型参数量C和数据大小D需要同步放大，但模型和数据分别放大的比例还存在争议
* Scaling Law不仅适用于语言模型，还适用于其他模态以及跨模态的任务
$$ L(x) = L_{\infty} + (\frac{x_0}{x})^\alpha$$
* 第一项$L_{\infty}$是指无法通过增加模型规模来减少的损失，可以认为是数据自身的熵（例如数据中的噪音）
* 第二项是指能通过增加计算量来减少的损失，可以认为是模型拟合的分布与实际分布之间的差。
* 根据公式，增大x(例如计算量C)，模型整体loss下降，模型性能提升；伴随x趋向于无穷大，模型能完美拟合数据的真实分布，让第二项逼近0，整体趋向于$L_{\infty}$

## 大模型参数头研究
### Bert类Attention Head总结
 [Revealing the Dark Secrets of BERT](https://arxiv.org/pdf/1908.08593)里面对 BERT 模型的自注意力机制进行了深入分析，
论文概述：揭示 BERT 的“黑匣子”机制
BERT（Bidirectional Encoder Representations from Transformers）因其在多种 NLP 任务中取得的出色表现而备受关注。然而，其内部工作机制仍然不甚明了。​本研究通过对 BERT 的自注意力机制进行定性和定量分析，旨在揭示其成功的关键因素。​

#### 🔍 主要发现
* 注意力模式的重复性：​研究发现，BERT 的多个注意力头之间存在重复的注意力模式，表明模型可能存在过度参数化的问题。​

* 注意力头的重要性差异：​不同的注意力头对模型在不同任务中的性能影响各异。​有些注意力头对特定任务至关重要，而其他则可能冗余。​

* 禁用特定注意力头的影响：​令人意外的是，手动禁用某些注意力头反而提升了模型在特定任务上的性能，最高提升达 3.2%。​

#### 🧠 方法论
* 特征选择：​研究者选择了一组感兴趣的语言特征，作为分析的基础。
#### 结论
* 这项研究表明，BERT 的部分注意力头可能是冗余的，模型存在过度参数化的问题。​通过识别并禁用这些冗余的注意力头，可以在不增加模型复杂度的情况下提升性能。​这为优化 Transformer 架构提供了新的思路。
### 大模型Attention Head的总结
* [Attention Heads of Large Language Models: A Survey](https://arxiv.org/pdf/2409.03752)
    * 这篇综述论文系统地总结了LLMs中不同类型的注意力头，包括：
    * Memory Head：在推理过程中调用模型的参数化知识。
    * Constant Head：在多选任务中均匀分配注意力分数。
    * Single Letter Head：专注于单个候选答案。
    * Negative Head：在二元决策任务中偏向否定表达。 此外，还讨论了其他类型的注意力头，如处理稀有词汇的“Rare Words Head”、捕捉重复内容的“Duplicate Head”、识别句法结构的“Syntactic Head”等。

* [Pruning Attention Heads of Transformer Models Using A* Search](https://arxiv.org/pdf/2110.15225)
    * 在Bert模型上，这篇论文引入了一种基于A*搜索算法的剪枝方法，能够在不损失准确率的情况下，剪除多达40%的注意力头，从而压缩大型NLP模型。
* [BlockPruner: Fine-grained Pruning for Large Language Models](https://arxiv.org/abs/2406.10594#:~:text=We%20propose%20a%20novel%2C%20training-free%20structured%20pruning%20approach,each%20Transformer%20layer%20into%20MHA%20and%20MLP%20blocks.)
    * 在LLM领域引入上面的Pruning 的A* Search搜索算法来进行大模型的压缩。
* [SlimGPT: Layer-wise Structured Pruning for Large
Language Models](https://arxiv.org/pdf/2412.18110)
    * 该研究提出了一种分层剪枝方法，通过移除冗余的注意力头来提高Transformer模型的效率。实验表明，在WikiText-103语言建模基准测试中，剪枝后的模型在保持性能的同时，显著减少了计算负担。

* [Layer-wise Pruning of Transformer Attention Heads for Efficient Language Modeling](https://arxiv.org/pdf/2110.03252)
    * 在Bert该研究提出了一种分层剪枝方法，通过移除冗余的注意力头来提高Transformer模型的效率。实验表明，在WikiText-103语言建模基准测试中，剪枝后的模型在保持性能的同时，显著减少了计算负担。
* [Towards Lossless Head Pruning through Automatic Peer Distillation for Language Models](https://www.ijcai.org/proceedings/2023/0568.pdf)
    * 该研究提出了一种自动同伴蒸馏方法，通过回收被剪除注意力头的知识，实现无损的注意力头剪枝。实验结果显示，在GLUE基准测试的九个任务中，平均剪除超过58%的注意力头，且性能优于其他剪枝技术。


* [On the Role of Attention Heads in Large Language Model Safety](https://arxiv.org/pdf/2410.13708)
    * 这篇论文探讨了注意力头在LLMs安全性中的作用。研究发现，特定的“安全头”对模型的安全性有显著影响。禁用单个安全头会使模型对有害查询的响应增加16倍，尽管只修改了0.006%的参数。

## TODO
​在评估大型语言模型（LLMs）时，除了准确性之外，效率也是一个关键因素。​以下是关于7B模型参数来源、Qwen1.5B与Qwen7B的参数差异，以及通过剪枝注意力头来降低模型参数量的探讨。​

📊 7B模型的参数来源
    7B模型中的“7B”指的是模型大约拥有70亿（7,000,000,000）个参数。这些参数主要分布在以下几个组件中：​
    Transformer层数量（Depth）：​例如，LLaMA-7B模型包含32个Transformer层。
    注意力头数量（Heads）：​每层通常有32个注意力头。
    前馈网络维度（FFN Dimension）：​例如，LLaMA-7B的前馈网络维度为11008。
    嵌入层和输出层：​包括词嵌入和输出投影层。​

    这些组件共同构成了模型的总参数量。​[参数](https://blogs.novita.ai/unveiling-llm-pruner-techniques-doubling-inference-speed/?utm_source=chatgpt.com)

🔍 Qwen1.5B与Qwen7B的参数差异
    Qwen1.5B和Qwen7B模型的主要差异在于以下几个方面：​
    Transformer层数量：​Qwen1.5B可能包含较少的层数，例如12层，而Qwen7B可能包含32层。
    注意力头数量：​Qwen1.5B的每层注意力头数量可能较少，例如12个，而Qwen7B可能有32个。
    前馈网络维度：​Qwen1.5B的前馈网络维度可能较小，例如4096，而Qwen7B可能为11008。​
    这些结构上的差异导致了两者在参数量和性能上的不同。​

✂️ 剪枝注意力头的有效性
    剪枝注意力头是一种减少模型参数量和计算成本的方法。​研究表明，许多注意力头在模型性能中贡献有限，因此可以被剪除而不会显著影响模型的准确性。​
    冗余性分析：​研究发现，Transformer模型中的注意力头存在冗余性，许多头对模型性能的贡献较小。
    剪枝策略：​通过剪除这些冗余的注意力头，可以减少模型的参数量和计算成本。例如，某研究在LLaMA-7B模型中剪除部分注意力头后，模型的性能几乎没有下降。
    性能影响：​在剪枝后，模型在多个基准测试中的表现与原始模型相当，甚至在某些任务中表现更好。​
    然而，仅仅剪枝注意力头可能不足以显著减少模型的参数量和计算成本。​因为前馈网络（FFN）通常占据了模型的大部分参数和计算资源。​

🔄 其他剪枝方法
    除了剪枝注意力头之外，还有其他方法可以用于减少模型的参数量和提高效率：​
    剪枝前馈网络：​通过剪除前馈网络中的冗余神经元，可以进一步减少模型的参数量。
    剪枝Transformer层：​研究表明，剪除部分Transformer层（即减少模型的深度）可以在保持性能的同时减少参数量。
    联合剪枝：​结合剪枝注意力头、前馈网络和Transformer层，可以实现更高效的模型压缩。​

✅ 总结
    剪枝注意力头是一种有效的模型压缩方法，可以在保持模型性能的同时减少参数量和计算成本。​然而，为了实现更显著的压缩效果，建议结合剪枝前馈网络和Transformer层等方法。​通过综合应用这些技术，可以在不显著影响模型性能的前提下，显著提高模型的效率。
    
## Reference
* [推荐系统中的attention](https://cloud.tencent.com/developer/article/1739486)
* [对比学习损失函数](https://www.cnblogs.com/sddai/p/17415668.html)
* [浅聊对比学习方法](https://cloud.tencent.com/developer/article/2318554)
* [Attention 用于加权](https://www.bilibili.com/video/BV1Zu4m1u75U/?buvid=XU39B19351AC124E091A1EF018DB57263E157&from_spmid=main.my-favorite.0.0&is_story_h5=false&mid=zRRaDbuabi3ybmVevhYIIA%3D%3D&p=1&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=fba2e39d-0c84-48aa-8a39-2bc567b00111&share_source=WEIXIN&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1718909216&unique_k=fJcsRmk&up_id=3546611527453161)
* [解析大模型中的Scaling Law](https://zhuanlan.zhihu.com/p/667489780)
