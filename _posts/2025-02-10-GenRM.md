---
title: 「Paper Reading」 Generative Verifiers Reward Modeling as Next-Token Prediction
author: Fangzheng
date: 2025-07-27 19:06:00 +0800
categories: [Paper Reading, RLHF]
tags: [Algorithm,Paper Reading]
# pin: true
mermaid: true  #code模块
comments: true

math: true
# img_cdn: https://github.com/MysteriousL2019/mysteriousl2019.github.io/tree/master/assets/img/
---
# GenRW 一种新的奖励设计思路
* [Generative Verifiers: Reward Modeling as Next-Token Prediction](https://arxiv.org/abs/2408.15240)
* 在大型语言模型（LLMs）的研究中，验证器或奖励模型常被用于提升其推理性能。传统的Best-of-N 方法是让 LLM 生成 N 个候选解，再由验证器进行排序选优。然而，基于 LLM 的验证器通常作为判别分类器训练，未充分利用预训练 LLM 的文本生成能力。本文提出利用通用的下一个词元预测目标来训练验证器，即生成式验证器（GenRM），它在验证和生成解决方案上联合训练，相比标准验证器有诸多优势，在算法和数学推理任务中表现更优，且能利用合成验证理由识别数学问题中的细微错误，还能随模型大小和推理时间计算良好扩展。
### **研究背景与动机**

-   尽管LLMs能力强大，但在推理问题上常出错。Best-of-N方法是解决此问题的常用策略，但基于LLM的验证器通常作为判别模型训练，未利用LLMs的文本生成能力，如统一指令调整、思维链推理和利用额外推理时间计算等优势。而简单使用现成LLM作为评判者（LLM-as-a-Judge）的方法在推理任务中表现也不佳。
-   因此，本文提出训练GenRM，通过下一个词元预测利用LLMs的文本生成能力来改进验证过程。
-   如下图，GenRM的概览： 模型的输入为 问题 + 解决方案（不知正确与否） ，输出为 COT（判断过程）+最终判断（yes or no）。而与此为对比的是 OpenAI的 PRM800K模型，也即 Discriminative RM（只是一个简单的分类器）。

![](https://pic1.zhimg.com/v2-6128dbe453c1a72467e5a974eaa6a6f8_1440w.jpg)

GenRM概览：一个数学示例，其中 GenRM-CoT（仅在 GSM 上训练）检测到推理错误。做出的解决方案简化中间步骤的错误。判别 RM 和 GenRM-CoT 模型都只在GSM8K训练。在这种情况下，判别 RM 无法将解决方案归类为不正确，而 GenRM-CoT 利用COT抓住这个错误

### **生成式验证器（GenRM）方法**

![](https://pic1.zhimg.com/v2-d9bd1c7d16137099a6c62c5a49890d26_1440w.jpg)

方法概览

首先GenRM-COT是本文着重将的方案，在这个地方呢，COT会有很多条，因为一条COT的路径有可能是错的，所以需要投票，也就是多条COT+判断的路径。而文中的超参数K（路径数目）设置在了32是最好的结果，说实话这个数目已经不是几台A100能够跑起来的了。那么有了多条路径，最终的分数如何计算呢？首先，模型会拿出最后的Yes、No的概率值，然后根据概率值，取得一个平均数，就是最终Yes、No的概率了。

  

在此处船长要插句嘴，不是所有的任务都是Yes、No就能够判定的，比如文本创作任务。所以该方法的适应领域目前在二分类会多一些。

  

好的，然后我们接着往下看。接下来我们尝试去分析一下论文中提到的数据，看看这批数据究竟是如何构造的？ 数据来自于Google的代码分享，地址为：[https://github.com/genrm-star/genrm-critiques/tree/main/critiques](https://github.com/genrm-star/genrm-critiques/tree/main/critiques)

  

```json
{"question": "There were 349 pieces of candy in a bowl. Talitha took 108 pieces and Solomon took 153 pieces. 
How many pieces of candy remain in the bowl?", 

"question_id": 0, 

"model_output": "Talitha took 108 pieces and Solomon took 153 pieces. 
This means they took out a total of 108 + 153 = 261 pieces. There were 349 pieces in the beginning, 
so now there are 349 - 261 = 88 pieces left in the bowl. The answer is 88.", 

"model_output_id": 0, 

"extracted_answer": "88", 

"target": "88", 

"correctness": 1, 

"verifier_prompt": "You are a math teacher. Grade the Solution, verifying correctness step by step. Use Expected Answer to find any erroneous step in the Solution. At the end of the Solution verification, when you give your final grade, write it in the form \"Verification: Is the answer correct (Yes/No)? X\",  where X is either Yes or No.\n\nQuestion: There were 349 pieces of candy in a bowl. Talitha took 108 pieces and Solomon took 153 pieces. How many pieces of candy remain in the bowl?\nSolution: Talitha took 108 pieces and Solomon took 153 pieces. This means they took out a total of 108 + 153 = 261 pieces. There were 349 pieces in the beginning, so now there are 349 - 261 = 88 pieces left in the bowl. The answer is 88.\nExpected Answer: Talitha took 108 pieces and Solomon took 153, so in total they took 108 + 153 = 261 pieces. Since there were 349 pieces in the beginning, now there must be 349 - 261 = 88 pieces left in the bowl. The answer is 88.", 

"verifier_output": "**Verification:**\n\n1. Talitha took 108 pieces and Solomon took 153 pieces. This means they took out a total of 108 + 153 = 261 pieces. (Correct)\n\n\n2. There were 349 pieces in the beginning, so now there are 349 - 261 = 88 pieces left in the bowl. (Correct)\n\n\n3. The answer is 88. (Correct)\n\n\n**Expected Answer:**\n\nTalitha took 108 pieces and Solomon took 153, so in total they took 108 + 153 = 261 pieces. Since there were 349 pieces in the beginning, now there must be 349 - 261 = 88 pieces left in the bowl. The answer is 88.\n\n\n**Final Grade:**\n\nVerification: Is the answer correct (Yes/No)? Yes"}
```

数据的存储格式是json文件，我们着重进行数据的讲解：

-   **"question"**：记录了具体的数学问题，即 “碗里有 349 块糖果，塔莉莎拿了 108 块，所罗门拿了 153 块，碗里还剩多少块糖果？”
-   **"question\_id"**：问题的唯一标识，这里是 0。
-   **"model\_output"**：模型对该问题的解答过程和答案，详细说明了塔莉莎和所罗门拿走的糖果总数为 261 块，然后用最初的 349 块减去拿走的 261 块，得出剩余 88 块糖果，答案是 88。
-   **"model\_output\_id"**：模型输出的标识，这里也是 0。
-   **"extracted\_answer"**：从模型输出中提取出的答案，即 88。
-   **"target"**：预期的正确答案，这里同样是 88。
-   **"correctness"**：表示模型答案的正确性，1 表示正确。
-   **"verifier\_prompt"**：给验证者（这里假设是一个数学老师）的提示，要求其逐步验证解决方案的正确性，使用预期答案来查找解决方案中的任何错误步骤，并在验证结束时以特定格式给出最终成绩。
-   **"verifier\_output"**：验证者的输出，详细地对模型的解答步骤进行了验证，每一步都标记为正确，最后给出最终成绩，确认答案是正确的（Yes）。

接着我们着重关注在verifier\_output 上面：

* * *

> **验证：**  
> 塔莉莎拿了 108 块，所罗门拿了 153 块，这意味着他们总共拿走了 108 + 153 = 261 块。（正确）  
> 一开始有 349 块，所以现在碗里还剩下 349 - 261 = 88 块。（正确）  
> 答案是 88。（正确）  
> **预期答案：**  
> 塔莉莎拿了 108 块，所罗门拿了 153 块，所以他们总共拿走了 108 + 153 = 261 块。因为一开始有 349 块，所以现在碗里一定还剩下 349 - 261 = 88 块。答案是 88。  
> **最终成绩：**  
> 验证：答案是否正确（是 / 否）？是

* * *

  

可以看出，模型的输出是每个步骤每个步骤进行判断的，并且会判断的输出每个步骤的正确与否，然后最终输出分数。预期答案，这里说的不是特别明确，应该是GenRM模型把正确答案输出了一遍，因为Google 团队认为RM必须有作对这道题的能力才能很好的进行判断。

  

接下来就是训练数据的构造，我们的疑惑集中在 verifier\_output 这样的数据格式是如何生成的，是人写的吗？答案不是，是来自于合成数据。合成数据的Prompt如下图所示，输入部分除了 Question，以及待判定的Solution，还有一个参考的答案，也就是说我们有一个参考答案，来生成我们使用的COT合成数据。这点在除去Math的任务上是比较难做到的，因为数学可以确保答案的唯一性。所以，该方法使用 Gemini1.0 Pro 模型生成合成理由，并通过参考引导评分提高质量，同时过滤掉错误的解决方案。

  

![](https://pic1.zhimg.com/v2-6bfd677807771d696710a1a50774167c_1440w.jpg)

合成数据的Prompt设计

目前LLM的大部分工作都集中在了，数据构造方面，所以把数据集的形式，内容，搞清楚就可以了。下面船长分析一下实验的具体设置。那么，这种数据构造的方法有多少提效呢？论文在实验中给出了结果，可以看出，合成数据（绿色）与没有guidence的数据比起来，确实好了很多，但是距离oracle（人工标注）指导的结果相比，还是差了很多。

![](https://pica.zhimg.com/v2-04dcb701d20f35222e6b5245ac6c383e_1440w.jpg)

合成数据的优势

  

### 数据集介绍

### 1\. 算法推理相关数据集

-   **Last Letter Concatenation**：

-   **内容**：这是一个字符串操作任务，具体任务可能是对给定的字符串进行某种与最后一个字母相关的拼接操作。例如，对于输入的字符串列表，可能需要根据一定规则提取每个字符串的最后一个字母并进行拼接等操作。
-   **训练与评估**：在长度为 {2,3,4} 的单词列表上训练验证器，然后在长度为 {5,6} 的单词列表上评估其泛化能力。通过这种方式，可以测试模型在处理不同长度字符串任务时的性能和泛化效果，看其是否能够将在较短字符串上学习到的知识和技能应用到更长的字符串任务中。

-   **Word Sorting from Big-Bench（Big-Bench 中的单词排序任务）**：

-   **内容**：该数据集来自 Big-Bench，任务是对单词进行排序。可能是根据字母顺序、单词长度或其他相关规则对给定的单词列表进行排序。
-   **训练与评估**：同样在长度为 {2,3,4} 的单词列表上训练验证器，然后在长度为 {5,6} 的单词列表上评估泛化能力，以检验模型在不同长度单词排序任务上的表现和泛化性能。

### 2\. 数学推理相关数据集

-   **[GSM8K 数据集]**：

-   **内容**：这是一个用于小学水平数学问题的数据集，包含了各种类型的数学应用题，涵盖了加、减、乘、除等基本运算以及一些简单的数学概念和逻辑。
-   **训练与评估**：在该数据集上训练小学水平的数学验证器，然后在 GSM8K 测试集上对这些验证器进行评估，同时还会评估它们在更难的 MATH 数据集上从易到难的泛化能力。使用与 Lightman 等人在 2023 年相同的 500 个 MATH 问题的保留集来进行这部分的评估，以全面考察模型在不同难度数学问题上的表现和泛化性能。

-   **MATH 数据集**：

-   **内容**：这是一个难度较大的数学数据集，包含了从小学到高中水平的各种数学问题，涉及代数、几何、概率、数论等多个数学领域，问题的难度和复杂度都较高。
-   **评估作用**：用于评估在 GSM8K 数据集上训练的验证器在更具挑战性的数学问题上的泛化能力，看模型是否能够将在相对简单的 GSM8K 数据集上学到的数学推理能力迁移到更难的 MATH 数据集上，从而检验模型的鲁棒性和泛化性能。

### Baseline设置

1\. Discriminative RM（判别式奖励模型）或 ORM

-   **概述**：这是在推理任务的测试时重排序中训练验证器的主流方法，被用作主要的基线方法。
-   **原理**：通过某种方式训练验证器，使其能够对不同的解决方案进行判别，判断其正确性或质量高低，从而在测试时对生成的结果进行重新排序，筛选出更优的答案。例如，可能基于二元交叉熵损失等方式，直接为候选解决方案分配一个数值分数来表示其优劣程度。

2\. LLM - as - a - Judge（大型语言模型作为评判者）,也即Zero-shot

-   **概述**：使用现成的预训练大型语言模型（LLM）进行验证。
-   **操作方式**：利用思维链（CoT）提示来生成 32 个验证理由，这些理由用于正确性预测，然后通过多数投票的方式确定正确性答案。也就是说，让 LLM 根据给定的提示生成多个关于答案正确性的理由，再依据这些理由中多数所支持的答案来判定最终的正确性。

3\. [DPO]（偏好优化）

-   **概述**：这是一种偏好优化方法。
-   **训练方式**：按照 Hosseini 等人在 2024 年的研究，使用这种方法在包含正确和错误解决方案的偏好对上来训练验证器。即通过对正确和错误的解决方案进行偏好标注，让验证器学习到正确答案的特征和模式，从而能够更好地进行验证和判断。

4\. Self - consistency（自一致性）

-   **概述**：这是一种在测试时不使用验证器的简单方法。
-   **操作流程**：从 LLM 生成器中采样多个解决方案，然后选择最常见的答案作为最终结果。这种方法基于这样的假设：如果 LLM 生成的多个答案中某个答案出现的频率最高，那么它很可能是正确的答案，通过这种方式来提高结果的准确性和可靠性，而无需专门的验证器来进行判断。船长认为这种方法是最简单的，不需要判别器，多次生成结果取众数，这样来判断是否正确。

### 评估方法

-   **主要评估指标：Best - of - N 性能 （排序指标）**

-   **定义**：使用固定生成器（在第 2 节中有提及）结合学习到的验证器，计算解决问题的百分比来衡量。
-   **报告内容**：报告在测试集上的平均准确率。这个指标主要用于评估验证器从多个生成的解决方案中挑选出正确答案的能力，即对解决方案进行排序并选出正确答案的能力。

  

-   **另一个评估指标：测试 RM 准确性（正确与否）**

-   **定义**：衡量验证器是否能够准确地对正确和错误的解决方案进行分类。
-   **与 Best - of - N 的关系**：虽然这两个指标是相关的，但 RM 准确性仅评估验证器逐点的准确性，而 Best - of - N 则评估验证器对解决方案进行排序以选择正确答案的能力。

### 实验细节

-   **训练验证器的模型**：

-   使用开源权重的 Gemma 模型（Gemma Team 等人，2024a,b）。具体来说，对于算法任务，使用 Gemma - 2B 模型；对于 GSM8K 任务，使用 Gemma 2B、7B 和 Gemma - 2 9B 模型。

-   **生成解决方案以及用于 LLM - as - a - Judge 的模型**：

-   对于算法任务，使用 Gemma 2B 模型；对于 GSM8K 任务，使用 Gemini 1.0 Pro（Google 等人，2023）模型。

-   **合成验证 CoT方式**：

-   对于算法任务，通过编程方式生成预言理由（oracle rationales）。
-   对于 GSM8K 任务，使用 Gemini 1.0 Pro 结合参考引导评分（reference - guided grading）来生成合成理由。

  

### 实验结果

![](https://pic4.zhimg.com/v2-3716f77d1a198b788e92a08b4565f331_1440w.jpg)

实验结果图，可以看出GenRM-COT的效果是最好的。但是GenRM（没有COT）的效果只是略微好于Discriminative RM

  

![](https://pic4.zhimg.com/v2-9e10922946fc4c9c6f0817f8518bf86b_1440w.jpg)

采样结果图

本图片是在说，采样数目不同，对于最后预测结果的影响程度。而比较突出的事，相比于Discriminative RM，GenRM-COT的方法，提效明显，不需要采样很多次，已经能够达到最终的结果。但是虽然如此，真正应用的时候，采样数字接近32还是很好的（虽然很废卡）。

  

![](https://pic1.zhimg.com/v2-f49d54480d8d7c15a25410fcdc31209c_1440w.jpg)

SFT在有无正确解法的测试结果。结果证明，把正确解法加入SFT中训练能够达到最好的效果。

这张图片是想说，在训练的过程中，加入这个 正确的解法，在输出的过程中，有助于模型达到更好的结果。但是是否 Solution加入的越多越好呢？这点论文在附录中给出了解释：

  

![](https://pic4.zhimg.com/v2-7220183ee4e1b5c1c04bc5c8b28ac7df_1440w.jpg)

Solution 数据配比

-   当 λ = 0 时，即不添加生成损失（solution generation loss），解决问题的百分比约为 89%。
-   随着 λ 值的增加（从 1/4 到 1/3 再到 1/2），解决问题的百分比有所上升，在 λ = 1/4 和 λ = 1/3 时，都达到了约 90%，这是相对较高的水平。
-   然而，当 λ = 1 时，解决问题的百分比下降到约 88%。
-   此处还有个细节问题，λ 到底是数据占比，还是Loss函数占比呢？这点论文里面说的有点简单。看起来更像是数据的占比。

  

![](https://pic3.zhimg.com/v2-6189efb4cad84d49e9ab03105b47da14_1440w.jpg)

在添加了多条COT数据之后的效果提升

那么论文的训练数据是只有一条COT的，如果这条合成的COT数据是错误的该怎么办呢？论文给出了一个方案是，引入多条COT的路径，实验结果证明是可以提升效果的，而论文给出的原因为：“我们怀疑这是因为模型生成的理由可能包含错误，以至于对每个解决方案的多个理由进行训练会产生一种“集成”效应，这种效应可以防止对这些错误过度拟合”。

### **研究贡献与未来方向**

-   本文提出的 GenRM 将验证重铸为下一个词预测，比判别式验证器性能更优，解锁了思维链推理和利用推理时间计算的能力，统一了生成和验证，且证明了合成理由可用于训练 GenRM 识别数学推理错误。
-   未来可将该框架扩展到编码、对齐、文本到图像生成和开放式生成等任务，还可结合过程级监督、强化学习等进一步提高生成式验证器的准确性，以及探索与其他提升 LLM 的技术相结合。

### 辩证思考

现在我们分析一下论文存在的问题：

1.  推理时间显著增加，当推理字数增多之后，推理时间显著提升。
2.  合成数据的质量，依赖于有正确标准答案的情况，那么如果没有正确的解决方案呢？方法并没有给出很好的解决方案。

论文优势：

1.  COT+预测，提升预测准确性。
2.  所有实验基于开源模型做的，除去合成数据是有gemini 1.0 pro 产生。