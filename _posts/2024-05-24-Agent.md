---
title: AI Agent
author: Fangzheng
date: 2024-05-24 19:06:00 +0800
categories: [Generative AI, Artificial Intelligence]
tags: [Algorithm ]
# pin: true
mermaid: true  #code模块
comments: true

math: true
# img_cdn: https://github.com/MysteriousL2019/mysteriousl2019.github.io/tree/master/assets/img/
---
## 什么是Agent？为什么是Agent？ 
* 当我们将大型模型视为“核心调度器“时，它就变成了我们的Agent。借助任务规划、记忆及外部工具等能力，大型模型能够识别出应该执行的任务以及执行方式，从而实现自主决策。
* Agent可被视为“具备自主智能的实体”，也被广泛地称作智能体。当前的Agent主要被分为三种类型：单体Agent、多Agent协作（Multi-Agent）、以及与人交互的Agent。
## 从CoT到Agent
* 在深入理解思维链（Chain of Thought, CoT）之前，我们先来认识两个概念，即“语言智能”和“推理”。
* “语言智能”可以被看作是使用基于自然语言的概念去理解经验事物，并对概念间进行推理的能力。而“推理”更常被理解为根据已知前提推出新结论的过程，这通常涉及到多个步骤并形成了关键的“中间概念”，它们有助于解决复杂问题。
* 一个完整的CoT形式的Prompt通常由三部分组成：指令（Instruction）、逻辑依据（Rationale）和示例（Exemplars），比如：
* 指令（Instruction）:"请你解决这个复杂的数学问题：求一个立方体的对角线长度，它的边长为'a'。"
* 逻辑依据（Rationale）:"首先，我们知道直角三角形中的勾股定理，然后，这个问题可以通过勾股定理解决。先找出立方体的一个面（一个平方），然后找出该平方的对角线长度，这就形成了一个新的直角三角形，该三角形的边长包括立方体的边，面的对角线，及我们要找的立方体的对角线。"
* 示例（Exemplars）:"让我们来执行它。首先，一个立方体的一个边面是一个正方形，而正方形的对角线长度可以通过勾股定理求出，即d=sqrt(a^2 + a^2)= asqrt(2)。接下来，我们需要找出立方体的对角线，这会涉及到一个新的直角三角形，他的边长是a， asqrt(2) 和立方体的对角线。再次应用勾股定理，我们可以得出立方体对角线长度为 sqrt(a^2 + 2a^2)= asqrt(3)。"
## 为什么使用CoT
* 提升了大模型的推理能力：通过将复杂问题拆解为简单的子问题，CoT大大增强了模型的推理能力。它还尽可能地减少了模型在解决问题时忽视关键细节的现象，从而确保计算资源始终集中在解决问题的核心步骤上。
* 提高了模型的可解释性：与单纯输出结果的模型相比，CoT可以展示模型的解题过程，帮助我们更好地理解模型是如何工作的。同时，其解题步骤的输出也为我们提供了错误定位的依据。
* 增强了模型的可控性：CoT让模型一步步展示解题步骤，我们通过观察这些步骤可以更大程度地影响模型的问题解决过程，避免模型成为无法控制的“完全黑盒”。
* 提升了模型的灵活性：只需添加一句“Let's think step by step”，CoT方法便可在各种大型模型中使用。此外，CoT赋予模型的分步思考能力并不仅仅局限于“语言智能”，在科学应用与AI Agent构建等领域也均可发挥重要作用。
## 何时使用CoT
* CoT (Chain of Thought) 的运用能够有效提升模型解决复杂任务的效率和准确度。如何理解这种差异效应呢？
* 任务复杂度： 对于简单的任务，如查找特定信息或回答事实性问题，CoT的优势可能不明显，因为这些任务的解决方法直接且单一；然而，当面临复杂任务，例如编写全面的项目报告或进行复杂的数学求解，CoT的序列推理过程可以显著提升模型的表现，通过分解和逐步解答，大大提高了问题解决的准确性。
* 模型规模与算力： 此外，CoT的效益也与模型的规模以及可用的算力有关。对于较小的模型，它们可能没有足够的计算能力或内部表现形式空间来有效地实施CoT。相反，较大的模型，特别是那些配备有大量计算资源的模型，更能有效地运用CoT策略，因为他们可以在信息处理和理解中，通过持续的、连续的推理过程，更加深入地理解相关上下文。
* 任务之间的关联性：CoT策略也依赖于处理的多个子任务之间的关联性。如果这些子任务在概念性或主题性上有一定的关联性，那么通过CoT，模型就可以很自然地建立起这些基础知识和子任务之间的联系，“桥梁”作用显现，如联合乘法和加法运算规则解决更复杂的数学问题。这种关联性允许模型在解答过程中，把先前步骤的信息有效地用在后续步骤里，从而提高整体任务的解决效率。
## LLM多模态
* CLIP使用了对比学习的方法，即通过正样本（匹配的图像-文本对）和负样本（不匹配的图像-文本对）来训练模型。在训练过程中，模型会尝试最大化正样本对的相似度（比如通过计算余弦相似度），同时最小化负样本对的相似度。
    * CLIP模型在zero-shot学习中表现强大，可以直接用于zero-shot推理，比如将猫的图片emb后，将猪狗猫等类的文本描述也分别emb，计算图片和类别emb之间的相似度，从而进行分类。
    * CLIP-ViT-L/14模型的14表示每个patch的分辨率为14X14，比如在224x224像素的图像上，总共有(224 / 14) x (224 / 14) = 16 x 16 = 256个patch。
* LLaVA的模型结构非常简单，就是CLIP+LLM(Vicuna，LLaMA结构)，利用Vison Encoder将图片转换为[N=1, grid_H x grid_W, hidden_dim]的feature map，然后接一个插值层Projection W，将图像特征和文本特征进行维度对齐。经过Projection后，得到[N=1, grid_H x grid_W=image_seqlen, emb_dim]。然后将 image token embedding和text token embedding合并到一起，作为语言模型的输入，生成描述的文本。
* 与InstructBLIP或Qwen-VL在数亿甚至数十几亿的图像文本配对数据上训练的、专门设计的视觉重新采样器相比，LLaVA用的是最简单的LMM架构设计，只需要在600K个图像-文本对上，训练一个简单的完全连接映射层即可。
### [CLIP](https://arxiv.org/pdf/2103.00020.pdf)
* CLIP模型是一个双塔结构，包括一个文本编码器Text Encoder和一个图像编码器Image Encoder。训练数据集的形式为(image, text)，对于每个正确匹配的image和text，text是对image的一句正确描述。CLIP模型需要对(image, text)的数据对进行预测，即(image, text)匹配的为1，不匹配的为0。
    * Text Encoder: 对于每个句子, 将其编码成一个隐向量, $\\T_i$ 512 纬度，因此N个句子就有 [N,512]
    * Image Encoder 对于每张img, 将其编码成一个隐向量 $\\l_i$ 512 纬度，N个图片有[N,512]
* 由于Text Encoder和Image Encoder最后都是输出[N,512]的Tensor，因此可以很方便地计算images和texts两两之间的相似度。CLIP可以选在ResNet或ViT作为Backbone。实验表明，ViT的效果要好于ResNet。
### CLIP损失函数
* CLIP采用对称损失函数，简单来说，就是对相似度矩阵，分别从行方向和列方向计算loss，最后取两者的平均。
* 1. 图像到文本（Image-to-Text）：
* 对于每一个图像，模型尝试找出与之对应的文本描述。模型计算该图像特征向量与所有文本特征向量的相似度（通常使用点积），并通过softmax函数将这些相似度转换为概率分布。模型的目标是使得与当前图像真实对应的文本的概率最大化。这通常通过最小化交叉熵损失来实现，其中正样本是图像对应的真实文本。
* 2. 文本到图像（Text-to-Image）：
* 对于每一个文本描述，模型尝试找出与之对应的图像。这个过程与图像到文本非常相似，但方向相反。模型计算该文本特征向量与所有图像特征向量的相似度，并通过softmax函数转换为概率分布。模型的目标是使得与当前文本真实对应的图像的概率最大化。这也是通过最小化交叉熵损失来实现，其中正样本是文本对应的真实图像。

```python
# image_encoder - ResNet or Vision Transformer
# text_encoder - CBOW or Text Transformer
# I[n, h, w, c] - minibatch of aligned images
# T[n, l] - minibatch of aligned texts
# W_i[d_i, d_e] - learned proj of image to embed
# W_t[d_t, d_e] - learned proj of text to embed
# t - learned temperature parameter
# extract feature representations of each modality
I_f = image_encoder(I) #[n, d_i]
T_f = text_encoder(T) #[n, d_t]
# joint multimodal embedding [n, d_e]
I_e = l2_normalize(np.dot(I_f, W_i), axis=1)
T_e = l2_normalize(np.dot(T_f, W_t), axis=1)
# scaled pairwise cosine similarities [n, n]
logits = np.dot(I_e, T_e.T) * np.exp(t)
# symmetric loss function
labels = np.arange(n)
# 图像到文本的损失函数，第0维度即图片的行维度
loss_i = cross_entropy_loss(logits, labels, axis=0)
# 文本到图像的损失函数
loss_t = cross_entropy_loss(logits, labels, axis=1)
loss = (loss_i + loss_t)/2
```
### [LLava](https://llava-vl.github.io/)
* 多模态指令数据。当下关键的挑战之一是缺乏视觉与语言组成的指令数据。本文提出了一个数据重组方式，使用 ChatGPT/GPT-4 将图像 - 文本对转换为适当的指令格式；
* 大型多模态模型。研究者通过连接 CLIP 的开源视觉编码器和语言解码器 LLaMA，开发了一个大型多模态模型（LMM）—— LLaVA，并在生成的视觉 - 语言指令数据上进行端到端微调。实证研究验证了将生成的数据用于 LMM 进行 instruction-tuning 的有效性，并为构建遵循视觉 agent 的通用指令提供了较为实用的技巧。使用 GPT-4，本文在 Science QA 这个多模态推理数据集上实现了最先进的性能。
* 研究者向公众发布了以下资产：生成的多模式指令数据、用于数据生成和模型训练的代码库、模型检查点和可视化聊天演示。
* LLaVA模型的架构，是将一个预训练的视觉编码器（CLIP ViT-L/14）与一个大规模语言模型（Vicuna）连接在一起。
* 这两个模型通过一个简单的映射矩阵连接，这个矩阵负责将视觉和语言特征对齐或转换，以便在一个统一的空间内对它们进行操作。在多模态指令跟随数据集上，LLaVA表现出色，跟GPT-4相比，分数达到了85.1%。在Science QA上，LLaVA的准确率刷新了纪录，达到92.53%。
* 与InstructBLIP或Qwen-VL在数亿甚至数十几亿的图像文本配对数据上训练的、专门设计的视觉重新采样器相比，LLaVA用的是最简单的LMM架构设计，只需要在600K个图像-文本对上，训练一个简单的完全连接映射层即可。
* 对于输入图像 X_v，本文使用预训练的 CLIP 视觉编码器 ViT-L/14 进行处理，得到视觉特征 Z_v=g (X_v)。实验中使用的是最后一个 Transformer 层之前和之后的网格特征。本文使用一个简单的线性层来将图像特征连接到单词嵌入空间中。具体而言，应用可训练投影矩阵 W 将 Z_v 转换为语言嵌入标记 H_v，H_v 具有与语言模型中的单词嵌入空间相同的维度：
* $\begin{align}\mathbb{H}_{\mathrm{v}}\ =\mathbb{N}\cdot\mathbb{Z}_{\mathrm{v}}\end{align}$
* 在LLaVA中，Vision Encoder使用的是CLIP-ViT-L/14，并且，需要注意的是，LLaVA使用最后一层Transformer之前或之后的grid features作为图像表示，而不是CLIP最后的输出层。
* 总结：LLaVA的模型结构非常简单，就是CLIP+LLM(Vicuna，LLaMA结构)，利用Vison Encoder将图片转换为[N=1, grid_H x grid_W, hidden_dim]的feature map，然后接一个插值层Projection W，将图像特征和文本特征进行维度对齐。经过Projection后，得到[N=1, grid_H x grid_W=image_seqlen, emb_dim]。然后将 image token embedding和text token embedding合并到一起，作为语言模型的输入，生成描述的文本。
### LLaVA两阶段训练
* 阶段一：特征对齐预训练。由于从CLIP提取的特征与word embedding不在同一个语义表达空间，因此，需要通过预训练，将image token embedding对齐到text word embedding的语义表达空间。这个阶段冻结Vision Encoder和LLM模型的权重参数，只训练插值层Projection W的权重。
* 阶段二：端到端训练。这个阶段，依然冻结Vision Encoder的权重，训练过程中同时更新插值层Projection W和LLM语言模型的权重，训练考虑Multimodal Chatbot和Science QA两种典型的任务。

## Reference
* [LLava模型架构和训练过程 | CLIP模型](https://blog.csdn.net/qq_35812205/article/details/136586853)